{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler , OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = [[\"This was a great movie with a good cast, all of them hitting on allcylinders.\",\n",
    "            \"Even if you're a huge Sandler fan, please don't bother with thisextremely disappointing comedy!\",\n",
    "            \"A movie of outstanding brilliance and a poignant and unusual love story\",\n",
    "            \"I had the misfortune to watch this rubbish on Sky Cinema Max in a coldwinter night\",\n",
    "            \"I am at a distinct disadvantage here. I have not seen the first two moviesin this series\",\n",
    "            \"This program is a lot of fun and the title song is so catchy I can't get itout of my head\"]]\n",
    "#mengubah tipe data feature dan target dari object ke string\n",
    "feature = str(feature)\n",
    "target = str(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di bagian ini akan dilakukan case folding terhadap feature berupa membuat setiap huruf menjadi lower case dan membuang tanda tanda baca  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this was a great movie with a good cast, all of them hitting on allcylinders.', \"even if you're a huge sandler fan, please don't bother with thisextremely disappointing comedy!\", 'a movie of outstanding brilliance and a poignant and unusual love story', 'i had the misfortune to watch this rubbish on sky cinema max in a coldwinter night', 'i am at a distinct disadvantage here. i have not seen the first two moviesin this series', \"this program is a lot of fun and the title song is so catchy i can't get itout of my head\"]]\n"
     ]
    }
   ],
   "source": [
    "#Lower\n",
    "#digunakan untuk melowercase setiap character dalam string string, \n",
    "feature = feature.lower()\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this was a great movie with a good cast all of them hitting on allcylinders even if youre a huge sandler fan please dont bother with thisextremely disappointing comedy a movie of outstanding brilliance and a poignant and unusual love story i had the misfortune to watch this rubbish on sky cinema max in a coldwinter night i am at a distinct disadvantage here i have not seen the first two moviesin this series this program is a lot of fun and the title song is so catchy i cant get itout of my head\n"
     ]
    }
   ],
   "source": [
    "#replace\n",
    "#digunakan untuk mengereplace character character yang tidak di inginkan di dalam string\n",
    "feature = feature.replace(\".\",\"\")\n",
    "feature = feature.replace(\"?\",\"\")\n",
    "feature = feature.replace(\"'\",\"\")\n",
    "feature = feature.replace(\",\",\"\")\n",
    "feature = feature.replace(\"!\",\"\")\n",
    "feature = feature.replace(\"``\",\"\")\n",
    "feature = feature.replace(\")\",\"\")\n",
    "feature = feature.replace(\"''\",\"\")\n",
    "feature = feature.replace('\"','')\n",
    "feature = feature.replace('(','')\n",
    "feature = feature.replace('[','')\n",
    "feature = feature.replace(']','')\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "di bagian ini akan dilakukan tokenization. Tokenization berfungsi untuk memisahkan setiap kata,tanda baca, simbol dan entitas\n",
    "dalam sebuah kalimat. nantinya hasil pemisahan ini yang di sebut token akan digunakan untuk analisa nantinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'a', 'great', 'movie', 'with', 'a', 'good', 'cast', 'all', 'of', 'them', 'hitting', 'on', 'allcylinders', 'even', 'if', 'youre', 'a', 'huge', 'sandler', 'fan', 'please', 'dont', 'bother', 'with', 'thisextremely', 'disappointing', 'comedy', 'a', 'movie', 'of', 'outstanding', 'brilliance', 'and', 'a', 'poignant', 'and', 'unusual', 'love', 'story', 'i', 'had', 'the', 'misfortune', 'to', 'watch', 'this', 'rubbish', 'on', 'sky', 'cinema', 'max', 'in', 'a', 'coldwinter', 'night', 'i', 'am', 'at', 'a', 'distinct', 'disadvantage', 'here', 'i', 'have', 'not', 'seen', 'the', 'first', 'two', 'moviesin', 'this', 'series', 'this', 'program', 'is', 'a', 'lot', 'of', 'fun', 'and', 'the', 'title', 'song', 'is', 'so', 'catchy', 'i', 'cant', 'get', 'itout', 'of', 'my', 'head']\n"
     ]
    }
   ],
   "source": [
    "#memisahkan setiap kata,simbol,tanda baca dan lain lain\n",
    "featureword = nltk.tokenize.word_tokenize(feature)\n",
    "print(featureword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 8), ('this', 4), ('of', 4), ('i', 4), ('and', 3), ('the', 3), ('movie', 2), ('with', 2), ('on', 2), ('is', 2), ('was', 1), ('great', 1), ('good', 1), ('cast', 1), ('all', 1), ('them', 1), ('hitting', 1), ('allcylinders', 1), ('even', 1), ('if', 1), ('youre', 1), ('huge', 1), ('sandler', 1), ('fan', 1), ('please', 1), ('dont', 1), ('bother', 1), ('thisextremely', 1), ('disappointing', 1), ('comedy', 1), ('outstanding', 1), ('brilliance', 1), ('poignant', 1), ('unusual', 1), ('love', 1), ('story', 1), ('had', 1), ('misfortune', 1), ('to', 1), ('watch', 1), ('rubbish', 1), ('sky', 1), ('cinema', 1), ('max', 1), ('in', 1), ('coldwinter', 1), ('night', 1), ('am', 1), ('at', 1), ('distinct', 1), ('disadvantage', 1), ('here', 1), ('have', 1), ('not', 1), ('seen', 1), ('first', 1), ('two', 1), ('moviesin', 1), ('series', 1), ('program', 1), ('lot', 1), ('fun', 1), ('title', 1), ('song', 1), ('so', 1), ('catchy', 1), ('cant', 1), ('get', 1), ('itout', 1), ('my', 1), ('head', 1)]\n"
     ]
    }
   ],
   "source": [
    "#memunculkan frekeunsi keluarnya kata\n",
    "frequency = nltk.FreqDist(featureword)\n",
    "print(frequency.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this was a great movie with a good cast all of them hitting on allcylinders even if youre a huge sandler fan please dont bother with thisextremely disappointing comedy a movie of outstanding brilliance and a poignant and unusual love story i had the misfortune to watch this rubbish on sky cinema max in a coldwinter night i am at a distinct disadvantage here i have not seen the first two moviesin this series this program is a lot of fun and the title song is so catchy i cant get itout of my head']\n"
     ]
    }
   ],
   "source": [
    "#memisahkan setiap kalimat dalam feature feature\n",
    "featurekalimat = nltk.tokenize.sent_tokenize(feature)\n",
    "print(featurekalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di bagian ini akan di lakukan filtering. Filtering dalam hal ini di bagi menjadi 2 yakni stoplist dan wordlist. stoplist adalah dimana kita membuang kata kata tidak penting seperti a,with yang merupakan stopword dalam bahasa inggris.sedangkan untuk wordlist adalah dimana kita menyimpan kata kata penting yang kita perlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'movie', 'good', 'cast', 'hitting', 'allcylinders', 'even', 'youre', 'huge', 'sandler', 'fan', 'please', 'dont', 'bother', 'thisextremely', 'disappointing', 'comedy', 'movie', 'outstanding', 'brilliance', 'poignant', 'unusual', 'love', 'story', 'misfortune', 'watch', 'rubbish', 'sky', 'cinema', 'max', 'coldwinter', 'night', 'distinct', 'disadvantage', 'seen', 'first', 'two', 'moviesin', 'series', 'program', 'lot', 'fun', 'title', 'song', 'catchy', 'cant', 'get', 'itout', 'head']\n"
     ]
    }
   ],
   "source": [
    "#dibagian ini dilakukan stopword removal untuk membuang stopword stopword seperti a, with yang dapat menganggu training model kita\n",
    "Stopwords =  set(stopwords.words('english'))\n",
    " \n",
    "featureremoval = []\n",
    "for i in featureword:\n",
    "    if i not in Stopwords:\n",
    "        featureremoval.append(i)\n",
    " \n",
    "print(featureremoval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di bagian ini akan dilakukan stemming. stemming adalah suatu proses dimana kita membuang infeksi kata untuk menjadikan kata terseut menjadi kata dasarnya. Walaupun kata dengan infeksi tersebut tidak memiliki arti yang sama dengan kata dasarnya. Hal ini dilakukan untuk memperlancar model untuk melakukan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great = great\n",
      "movie = movi\n",
      "good = good\n",
      "cast = cast\n",
      "hitting = hit\n",
      "allcylinders = allcylind\n",
      "even = even\n",
      "youre = your\n",
      "huge = huge\n",
      "sandler = sandler\n",
      "fan = fan\n",
      "please = pleas\n",
      "dont = dont\n",
      "bother = bother\n",
      "thisextremely = thisextrem\n",
      "disappointing = disappoint\n",
      "comedy = comedi\n",
      "movie = movi\n",
      "outstanding = outstand\n",
      "brilliance = brillianc\n",
      "poignant = poignant\n",
      "unusual = unusu\n",
      "love = love\n",
      "story = stori\n",
      "misfortune = misfortun\n",
      "watch = watch\n",
      "rubbish = rubbish\n",
      "sky = sky\n",
      "cinema = cinema\n",
      "max = max\n",
      "coldwinter = coldwint\n",
      "night = night\n",
      "distinct = distinct\n",
      "disadvantage = disadvantag\n",
      "seen = seen\n",
      "first = first\n",
      "two = two\n",
      "moviesin = moviesin\n",
      "series = seri\n",
      "program = program\n",
      "lot = lot\n",
      "fun = fun\n",
      "title = titl\n",
      "song = song\n",
      "catchy = catchi\n",
      "cant = cant\n",
      "get = get\n",
      "itout = itout\n",
      "head = head\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "for k in featureremoval: \n",
    "    print(k, \"=\", ps.stem(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeeding adalah suatu proses dimana kita mengkonversi sebuah teks menjadi angka dalam bentuk vektor. hal ini dilakukan karena sebagian besar algoritma dalam python tidak memiliki kemampuan untuk melakukan training terhadap string dan text. pada soal ini telah kita lakukan casefolding menjadi kedalam bentuk kata kata dan kemudian kata kata tersebut akan di konversi kedalam bentuk numerik vektor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disini saya kembali mendeclare setiap kata kata dikarenakan jika saya langsung menggunakan featureremoval dari bagian stemming maka lenghtnya akan menjadi sebanyak kata kata bukan sebanyak kalimat yang ada\n",
    "# featurecasefold = np.array([ ['great', 'movi', 'good', 'cast', 'hit','cylind'],\n",
    "#                              ['even', 'your', 'huge', 'sandler', 'fan', 'pleas', 'dont', 'bother', 'extreme', 'disappoint', 'comedi'],\n",
    "#                              ['movi', 'outstand', 'brillianc', 'poignant', 'unusu', 'love', 'stori'],\n",
    "#                              ['misfortun', 'watch', 'rubbish', 'sky', 'cinema', 'max', 'coldwint', 'night'],\n",
    "#                              ['distinct', 'disadvantag', 'seen', 'first', 'two', 'movi', 'seri'],\n",
    "#                              ['program', 'lot', 'fun', 'titl', 'song', 'catchi', 'cant', 'get', 'head']])\n",
    "featurecasefold = np.array([ ['great movi good cast hit cylind'],\n",
    "                             ['even your huge sandler fan pleas dont bother extreme disappoint comedi'],\n",
    "                             ['movi outstand brillianc poignant unusu love stori'],\n",
    "                             ['misfortun watch rubbish sky cinema max coldwint night'],\n",
    "                             ['distinct disadvantag seen first two movi seri'],\n",
    "                             ['program lot fun titl song catchi cant get head']])\n",
    "target = np.array([ [\"Positive\"],\n",
    "          [\"Negative\"],\n",
    "          [\"Positive\"],\n",
    "          [\"Negative\"],\n",
    "          [\"Negative\"],\n",
    "          [\"Positive\"] ])\n",
    "# featurecasefold = np.array(featurecasefold)\n",
    "# target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'numpy.ndarray'>\n",
      "Length:  6\n",
      "Type:  <class 'numpy.ndarray'>\n",
      "Length:  6\n"
     ]
    }
   ],
   "source": [
    "#print tipe dari feature dan banyak featurenya\n",
    "print('Type: ', type(featurecasefold))\n",
    "print('Length: ', len(featurecasefold))\n",
    "print('Type: ', type(target))\n",
    "print('Length: ', len(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=2, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#model implementation\n",
    "#model ini akan menggunakan hierarchical softmax untuk mengtrain data akan memiliki ukuran feature sebesar 100. ini membuat model ini akan memiliki hierarchial output dan menggunakan softmax function pada layer terakhirnya\n",
    "modelfeature = Word2Vec(sentences = featurecasefold, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "modelfeature.init_sims(replace = True)\n",
    "modelfeature.save('word2vecmodelfeature')\n",
    "modelfeature = Word2Vec.load('word2vecmodelfeature')\n",
    "print(modelfeature)\n",
    "modeltarget = Word2Vec(sentences = target, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "modeltarget.init_sims(replace = True)\n",
    "modeltarget.save('word2vecmodeltarget')\n",
    "modeltarget = Word2Vec.load('word2vecmodeltarget')\n",
    "print(modeltarget)\n",
    "#print vocab yang ada di bawah berguna untuk mengetahui beragam macam string yang ada, seperti contohnya untuk target hanya ada 2 macam yaitu positive dan negative maka dari itu vocabnya hanya ada 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14139578 -0.15452746 -0.10391676  0.00444754 -0.02853733 -0.10024175\n",
      "  -0.07323908 -0.0310267   0.08809736  0.02405438 -0.002307    0.02654263\n",
      "  -0.02779163 -0.00429139  0.10922731  0.10282768 -0.06354536 -0.11234605\n",
      "  -0.10737465 -0.04549555 -0.02415729  0.05853832  0.1649285  -0.02536117\n",
      "  -0.05525004 -0.0293949  -0.15411346  0.1519554   0.02964898 -0.16430447\n",
      "   0.0867385   0.00454118 -0.11707285  0.04211499 -0.04840746  0.01044454\n",
      "  -0.17067054  0.02813452 -0.05965364 -0.18040541  0.04051857 -0.05972388\n",
      "  -0.00956252  0.01608486  0.08258258 -0.16327219 -0.14363325 -0.00535721\n",
      "   0.1401946  -0.18016003  0.00566673 -0.16348538 -0.09900035  0.12174911\n",
      "   0.01890818 -0.14724083 -0.08860787 -0.03583032 -0.05488735 -0.04508565\n",
      "  -0.08790813 -0.11051644  0.06770249 -0.01351312 -0.13169903 -0.04783081\n",
      "   0.04022935 -0.17644376 -0.17518418 -0.06456487 -0.07130415 -0.06696139\n",
      "  -0.09849687  0.08353431 -0.04065412  0.16618454 -0.04925666  0.02939223\n",
      "  -0.15605785 -0.09468408 -0.12717703 -0.10570201 -0.00685568 -0.10545027\n",
      "   0.13432601  0.00321924 -0.06526143  0.10611719 -0.10330741 -0.05908957\n",
      "   0.16617867 -0.0255723   0.12205186 -0.14611724  0.08687627  0.17346813\n",
      "  -0.10588893 -0.13902894 -0.12368502  0.16061413]\n",
      " [-0.00522727 -0.07926606  0.09366103  0.10653378  0.0748826   0.10399112\n",
      "  -0.15105438  0.0116412  -0.10184339 -0.01433725 -0.07467911  0.11619934\n",
      "   0.01424401  0.14333658 -0.13845982  0.01521046  0.16195053 -0.04475301\n",
      "   0.07072099  0.14860894  0.0799363   0.12368463  0.15889892  0.13289545\n",
      "  -0.1116021   0.12110867 -0.05368302  0.07997931 -0.07197424 -0.12188318\n",
      "  -0.12542535  0.08014725 -0.00482251  0.04696707 -0.03774552  0.16888385\n",
      "  -0.07426058 -0.11181512 -0.07348918  0.05632791  0.09357057 -0.06877842\n",
      "   0.04683062 -0.17138381  0.01374008 -0.16348685  0.07933642  0.00762035\n",
      "  -0.01613443 -0.10388233  0.132552    0.08258815 -0.14870939 -0.13081777\n",
      "   0.04517899 -0.12340163 -0.01499443  0.15154524  0.05552248 -0.05677436\n",
      "  -0.07170819 -0.04859894 -0.09052357 -0.1253719   0.13587902 -0.09760979\n",
      "   0.06520829 -0.133045   -0.11333746 -0.10497686 -0.0891654  -0.01379431\n",
      "  -0.09534176 -0.0956327  -0.03137163  0.16569546 -0.10020985 -0.15741728\n",
      "  -0.16122769 -0.03765871  0.02832245  0.12176593 -0.02411491 -0.10307146\n",
      "  -0.12956451 -0.11015994  0.1254268   0.03588421  0.07849661  0.04872357\n",
      "   0.09401815 -0.06622513  0.14629532 -0.1195959  -0.0221962   0.14086406\n",
      "   0.13605559  0.06175055 -0.01899177  0.10870492]\n",
      " [ 0.08623929 -0.09284025  0.0670691   0.1030371  -0.16247974 -0.0273769\n",
      "  -0.00953501 -0.10841173  0.06139658  0.16389562 -0.02243555  0.12861972\n",
      "   0.16803314 -0.08511551  0.03430665  0.07592504 -0.05706628  0.01197164\n",
      "  -0.00553185 -0.02928613  0.08403952  0.06230323 -0.15604164 -0.08153142\n",
      "   0.12891649  0.10220159  0.00899185 -0.09027687  0.12691055  0.03032459\n",
      "  -0.11372581 -0.15003078 -0.0481413   0.08909357 -0.13124685 -0.12395703\n",
      "   0.08164581  0.13685094  0.05923157 -0.11762685  0.06690802 -0.06530286\n",
      "   0.14855123 -0.13944045 -0.00828878  0.10868574 -0.05000886  0.15834059\n",
      "   0.1710686  -0.00141626  0.12152696 -0.04032517 -0.03918024 -0.1343671\n",
      "  -0.01694311 -0.1277592  -0.0990415   0.11351068  0.01516461  0.1644218\n",
      "   0.024554   -0.04547355  0.03056807  0.10621201 -0.06690881  0.12532592\n",
      "   0.10440814  0.0212282   0.08755071 -0.04159122 -0.01678947  0.15606019\n",
      "   0.07802088  0.10277216 -0.008628   -0.11958202  0.11385582 -0.14500287\n",
      "   0.0192588   0.03430988  0.0941667  -0.03938647  0.12982628 -0.06202488\n",
      "   0.16284771 -0.04720574  0.15835492 -0.05935142  0.1286814  -0.07229326\n",
      "  -0.09030604 -0.07065274 -0.01698308 -0.16973856 -0.15693714 -0.050986\n",
      "   0.15485099 -0.13568783 -0.162245    0.05637847]\n",
      " [ 0.10567541 -0.04353682  0.15861541 -0.00814899  0.14633441 -0.05246263\n",
      "   0.12229787 -0.08414011 -0.0824256  -0.18177174  0.02483026  0.1097633\n",
      "   0.09387554  0.08300731 -0.12095815 -0.05373228  0.17421304  0.16245782\n",
      "   0.06984007  0.13053949 -0.06512281 -0.08941015  0.0583966   0.09850229\n",
      "  -0.00978579 -0.13972212 -0.10796805  0.01648434  0.08343893 -0.12734903\n",
      "   0.04866249 -0.13002728 -0.08283088  0.09956615  0.01269012 -0.04144839\n",
      "  -0.06056692 -0.02615775  0.07024027  0.04015296 -0.06465634  0.16173178\n",
      "  -0.09371428  0.08701412  0.0916914  -0.04413571  0.14640951  0.02094573\n",
      "  -0.03945775 -0.11076318 -0.01329342  0.11017144 -0.14799531 -0.15666549\n",
      "  -0.09601346  0.13938747  0.04805416  0.09425593  0.05355484 -0.15280385\n",
      "  -0.16984867  0.10995971 -0.12716465  0.10315201  0.15536775 -0.07413825\n",
      "  -0.08057698 -0.07958841  0.00718095 -0.05971356  0.15397929 -0.12566467\n",
      "  -0.10859326 -0.08157434 -0.12129974 -0.04864085  0.11812673 -0.02364045\n",
      "  -0.09525185  0.03997955  0.02063655  0.12318721  0.03658067  0.0762581\n",
      "  -0.05284665  0.01164705 -0.16389465  0.09166245 -0.00761827  0.14316459\n",
      "   0.12216781 -0.1210108   0.06972872  0.11543234 -0.1553716  -0.05712954\n",
      "   0.09106699 -0.13187213 -0.0659313   0.06267436]\n",
      " [-0.03930442  0.09343667 -0.13316153 -0.1023953  -0.1081292  -0.07921428\n",
      "  -0.09163259 -0.09638981 -0.05895423  0.04723858 -0.14948307  0.10516555\n",
      "  -0.00670913 -0.07066668 -0.00222111  0.11987852 -0.00147731  0.08408122\n",
      "   0.12417443  0.14164469  0.03896968  0.13378154 -0.09642021 -0.14138849\n",
      "   0.13026525 -0.07112732 -0.0100335   0.10332147  0.10689106 -0.09409185\n",
      "   0.14438917 -0.12810779  0.1327735   0.15501085  0.07981058  0.12660852\n",
      "  -0.10471603 -0.06286327 -0.07900877  0.09525187 -0.11254631  0.1394213\n",
      "   0.08857789  0.04123874  0.14042598 -0.15431455 -0.14928854 -0.03432566\n",
      "  -0.12411962  0.14899117 -0.05843244 -0.10557757  0.01793034  0.1389122\n",
      "  -0.01673178 -0.12522632  0.14976367 -0.15349573  0.06586923  0.03942409\n",
      "   0.0736277   0.06313233  0.06124118 -0.14377119 -0.09190832 -0.06359879\n",
      "   0.15726146  0.08503664  0.06721874 -0.11358652 -0.05990021  0.07138699\n",
      "  -0.14725041 -0.12602565 -0.00707078 -0.14961821  0.1323088  -0.00737192\n",
      "   0.07330885  0.0962267  -0.12214261  0.07081561  0.05486788 -0.00653554\n",
      "  -0.03297435 -0.09674717  0.01454381  0.0713869  -0.09518225  0.08526433\n",
      "   0.03309476  0.02519631 -0.06973268 -0.14552666 -0.04095369  0.026214\n",
      "  -0.04827275 -0.1551815   0.13987447  0.12408067]\n",
      " [-0.00293916 -0.07539658 -0.05815251  0.14716384 -0.1113455  -0.16027647\n",
      "   0.00683206 -0.06031554  0.15678015  0.10349341 -0.10557243 -0.06049336\n",
      "   0.12270152  0.03914114  0.160092    0.09807167 -0.02170226 -0.15481727\n",
      "   0.0018529   0.02022594  0.08107289  0.1548451  -0.05276665 -0.06777936\n",
      "  -0.04260245 -0.04205646  0.14097886 -0.0813415   0.10348505  0.04856439\n",
      "   0.11925274  0.11130152 -0.05067793 -0.13360366  0.03744801 -0.15342477\n",
      "   0.13299264 -0.00443881  0.01261519 -0.12533602 -0.0148687   0.01786247\n",
      "   0.16141032 -0.0975419   0.11008082  0.03757384  0.1512553   0.14278048\n",
      "   0.10611767  0.1438114  -0.09480825 -0.0718274  -0.00836352 -0.06517179\n",
      "  -0.07101376 -0.16682576 -0.03281988  0.1170992   0.15356457  0.13411579\n",
      "   0.00822384 -0.07081901 -0.07477719 -0.00383055  0.07628421 -0.13232608\n",
      "   0.14122228  0.06017566 -0.06006509 -0.10765998  0.11880229 -0.14934266\n",
      "   0.01487828  0.13638617 -0.00353676  0.1257421   0.09773314  0.06467853\n",
      "  -0.10891855 -0.16215847 -0.08728547  0.14123838 -0.153423    0.07852369\n",
      "  -0.01824101 -0.02282873 -0.08485792 -0.0577344  -0.1318056   0.14372706\n",
      "  -0.05537076  0.09482634 -0.00890204  0.06486001 -0.1641358  -0.05100298\n",
      "  -0.0935391  -0.15371455 -0.0010461   0.029382  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vektor yang diprint digunakan untuk mempresentasikan setiap kata kedalam bentuk vektornya\n",
    "featurenew = modelfeature[modelfeature.wv.vocab]\n",
    "print(featurenew)\n",
    "#normalize\n",
    "Scaler = MinMaxScaler()\n",
    "featurenew = Scaler.fit_transform(featurenew)\n",
    "#disini data di transpose agar kolom dari feature dan target sama\n",
    "featurenew = featurenew.transpose()\n",
    "featurenew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vektor untuk target yang hanya terdiri dari 2 vocab yakni positif dan negatif\n",
    "targetnew = modeltarget[modeltarget.wv.vocab]\n",
    "#normalize\n",
    "Scaler = MinMaxScaler()\n",
    "targetnew = Scaler.fit_transform(targetnew)\n",
    "#disini data di tranpose agar kolom dari feature dan target sama\n",
    "targetnew = targetnew.transpose()\n",
    "targetnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Error: 0.15904735028743744 \n",
      "Epoch: 200 Error: 0.1321330964565277 \n",
      "Epoch: 300 Error: 0.12950745224952698 \n",
      "Epoch: 400 Error: 0.1287432610988617 \n",
      "Epoch: 500 Error: 0.12811943888664246 \n",
      "Epoch: 600 Error: 0.12758156657218933 \n",
      "Epoch: 700 Error: 0.12711532413959503 \n",
      "Epoch: 800 Error: 0.12670888006687164 \n",
      "Epoch: 900 Error: 0.12635202705860138 \n",
      "Epoch: 1000 Error: 0.12603603303432465 \n",
      "Epoch: 1100 Error: 0.12575359642505646 \n",
      "Epoch: 1200 Error: 0.12549862265586853 \n",
      "Epoch: 1300 Error: 0.12526610493659973 \n",
      "Epoch: 1400 Error: 0.12505188584327698 \n",
      "Epoch: 1500 Error: 0.12485256046056747 \n",
      "Epoch: 1600 Error: 0.1246652752161026 \n",
      "Epoch: 1700 Error: 0.12448771297931671 \n",
      "Epoch: 1800 Error: 0.12431792169809341 \n",
      "Epoch: 1900 Error: 0.12415430694818497 \n",
      "Epoch: 2000 Error: 0.12399550527334213 \n",
      "Epoch: 2100 Error: 0.12384037673473358 \n",
      "Epoch: 2200 Error: 0.1236879825592041 \n",
      "Epoch: 2300 Error: 0.12353750318288803 \n",
      "Epoch: 2400 Error: 0.12338823080062866 \n",
      "Epoch: 2500 Error: 0.12323962152004242 \n",
      "Epoch: 2600 Error: 0.12309114634990692 \n",
      "Epoch: 2700 Error: 0.12294242531061172 \n",
      "Epoch: 2800 Error: 0.1227930411696434 \n",
      "Epoch: 2900 Error: 0.12264275550842285 \n",
      "Epoch: 3000 Error: 0.1224912628531456 \n",
      "Epoch: 3100 Error: 0.12233839184045792 \n",
      "Epoch: 3200 Error: 0.12218393385410309 \n",
      "Epoch: 3300 Error: 0.12202775478363037 \n",
      "Epoch: 3400 Error: 0.121869757771492 \n",
      "Epoch: 3500 Error: 0.12170982360839844 \n",
      "Epoch: 3600 Error: 0.12154793739318848 \n",
      "Epoch: 3700 Error: 0.12138403952121735 \n",
      "Epoch: 3800 Error: 0.12121812254190445 \n",
      "Epoch: 3900 Error: 0.12105019390583038 \n",
      "Epoch: 4000 Error: 0.12088026851415634 \n",
      "Epoch: 4100 Error: 0.1207084208726883 \n",
      "Epoch: 4200 Error: 0.12053465843200684 \n",
      "Epoch: 4300 Error: 0.12035908550024033 \n",
      "Epoch: 4400 Error: 0.12018179893493652 \n",
      "Epoch: 4500 Error: 0.1200028657913208 \n",
      "Epoch: 4600 Error: 0.11982238292694092 \n",
      "Epoch: 4700 Error: 0.11964050680398941 \n",
      "Epoch: 4800 Error: 0.11945731937885284 \n",
      "Epoch: 4900 Error: 0.11927296966314316 \n",
      "Epoch: 5000 Error: 0.1190875917673111 \n",
      "Conf : [[9 1]\n",
      " [8 2]]\n",
      "Accuracy: 0.550000011920929\n",
      "recall:  0.5294117647058824\n",
      "precision: 0.9\n"
     ]
    }
   ],
   "source": [
    "#mengubah tf dari versi 2 ke versi 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# featurecasefold = featurecasefold.reshape(-1,1)\n",
    "# target = target.reshape(-1,1)\n",
    "\n",
    "# modelfeature = modelfeature.reshape(1,1)\n",
    "# modeltarget = modeltarget.reshape(1,1)\n",
    "\n",
    "#input adalah feature yang telah di word embeeding sebelumnya dengan nama featurenew\n",
    "#outputnya adalah target yang sebelumnya telah di word embeeding dengan nama targetnew\n",
    "\n",
    "#generate layer,weight and bias\n",
    "layer = {\n",
    "    'input' : 6,\n",
    "    'hidden' : 3,\n",
    "    'output' : 2\n",
    "}\n",
    "#init weight and bias random\n",
    "weight = {\n",
    "    'input_to_hidden' : tf.Variable(tf.random_normal([layer['input'], layer['hidden']])),\n",
    "    'hidden_to_output' : tf.Variable(tf.random_normal([layer['hidden'], layer['output']])) \n",
    "}\n",
    "bias = {\n",
    "    'input_to_hidden' : tf.Variable(tf.random_normal([layer['hidden']])),\n",
    "    'hidden_to_output' : tf.Variable(tf.random_normal([layer['output']]))\n",
    "}\n",
    "#Untuk mempermudah feed_dict di bawah\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, layer['input']])\n",
    "output_placeholder = tf.placeholder(tf.float32, [None, layer['output']])\n",
    "\n",
    "#feed forward function\n",
    "def feed_forward(featurenew):\n",
    "    weightxbias1 = tf.matmul(featurenew, weight['input_to_hidden']) + bias['input_to_hidden']\n",
    "    activation1 = tf.nn.sigmoid(weightxbias1)\n",
    "\n",
    "    weightxbias2 = tf.matmul(activation1, weight['hidden_to_output']) + bias['hidden_to_output']\n",
    "    activation2 = tf.nn.sigmoid(weightxbias2)\n",
    "\n",
    "    return activation2\n",
    "#generate learning rate and epoch\n",
    "learning_rate = 0.3\n",
    "epoch = 5000\n",
    "#train dataset using mean squared error and gradient descent optimizer\n",
    "output = feed_forward(input_placeholder)\n",
    "mse = tf.reduce_mean(0.5 * (output - output_placeholder) ** 2) \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(mse)\n",
    "saver = tf.train.Saver()\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(featurenew,targetnew,test_size=0.2)\n",
    "\n",
    "#print(feature_test)\n",
    "#data training\n",
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#declare feature dan target untuk data training\n",
    "    feed_train = {\n",
    "        input_placeholder: feature_train,\n",
    "        output_placeholder: target_train\n",
    "    }\n",
    "#mengprint error setiap 100 epoch\n",
    "    for i in range(epoch):\n",
    "        sess.run(train, feed_dict=feed_train)\n",
    "        error = sess.run(mse, feed_dict = feed_train)\n",
    "        #pred = sess.run(output, feed_dict = feed_train)\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {} Error: {} '. format(i+100, error))\n",
    "            #print('Pred : {}',pred)\n",
    "#declare feature dan target untuk data testing                  \n",
    "    feed_test = {\n",
    "        input_placeholder: feature_test,\n",
    "        output_placeholder: target_test\n",
    "    }\n",
    "#     for i in range(epoch):\n",
    "#         sess.run(train, feed_dict=feed_test)\n",
    "#         pred = sess.run(output, feed_dict = feed_test)\n",
    "#         if i % 100 == 0:\n",
    "#             print('Pred : {}',pred)\n",
    "#accuracy,precision dan recall  \n",
    "    matches = tf.equal(tf.argmax(output, axis=1),tf.argmax(output_placeholder,axis=1))\n",
    "    conf  = tf.math.confusion_matrix(tf.argmax(output_placeholder, axis=1),tf.argmax(output,axis=1))\n",
    "    print('Conf : {}'.format(sess.run(conf,feed_dict = feed_test)))\n",
    "#     matrix = format(sess.run(conf,feed_dict = feed_test))\n",
    "#     print(prediction)\n",
    "#     print(matches)\n",
    "    accuracy = tf.reduce_mean(tf.cast(matches,tf.float32))#coreect predicition / total prediction\n",
    "    precision = conf[0,0] / (conf[0,0] + conf[0,1])#true positive/ true positive+false positive\n",
    "    recall = conf[0,0] / (conf[0,0] + conf[1,0])#true positive/true positive + false negative\n",
    "    #recall = tf.metrics.recall(tf.argmax(output_placeholder, axis=1),tf.argmax(output,axis=1))\n",
    "   # precision = tf.metrics.precision(tf.argmax(input = output, axis=1),tf.argmax(output_placeholder,axis=1))\n",
    "    print('Accuracy: {}'.format(sess.run(accuracy, feed_dict =feed_test)))\n",
    "    print('recall: ', format(sess.run(recall,feed_dict = feed_test)))\n",
    "    print('precision:',format(sess.run(precision,feed_dict = feed_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision adalah suatu nilai dimana seluruh prediksi yang kita dapat adalah hasil yang relevan atau sesuai dengan target, tetapi tidak di ketahui apakah seluruh hasil yang relevan telah kita dapat atau tidak. sedangkan untuk recall adalah suatu nilai dimana seluruh nilai prediksi relevan telah kita ambil, namun tidak diketahui seberapa banyak data irrelevan yang telah kita ambil. Hal ini membuat nilai dari recall dan precision selalu bertolak belakang.Misalkan jika nilai recall naik maka nilai precision akan turun. Begitu juga sebaliknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penjelasan model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jadi dari data di atas kita dapat bahwa precision,recall, dan accuracy yang saya dapat selalu berubah ubah. Hal ini dikarenakan weight dan bias yang saya gunakan adalah random sehingga hal ini menyebabkan inkosistensi dari hasil saya ini. Selama saya menjalankan koding ini saya mendapat hasil bahwa accuracy yang di dapat berkisar antara 40%-60%, untuk recall di dapat recall sebesar 35% sampai dengan 65%,dan untuk precision berkisar dari 50% sampai dengan 100%. jadi dapat kita di lihat bahwa data ini memiliki High Precision + high recall artinya model ini tidak akan mengmiss classify hasil positive dan juga hasil negative dari data data di atas.dapat juga di artikan bahwa model ini \"Highly Depenable\" dan juga dengan akurasi yang berada disekitar 50% membuat model ini mampu untuk memprediksi data dari dataset di atas untuk sesuai dengan targetnya.Kesimpulannya model ini sangat cocok untuk memprediksi dataset yang kita gunakan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
